"""
ğŸ¬ Whisperè¯­éŸ³è¯†åˆ«ç®¡ç†æ¨¡å—
ç®¡ç†Whisperæ¨¡å‹åŠ è½½å’Œè¯­éŸ³è¯†åˆ«åŠŸèƒ½
"""

import logging
import os
import platform
import time
from pathlib import Path
from typing import Dict, Any, Optional, Union, Callable
import whisper
try:
    from .config import CONFIG
    from .utils import FileUtils, SystemUtils
except Exception:
    from config import CONFIG
    from utils import FileUtils, SystemUtils

logger = logging.getLogger(__name__)

class WhisperManager:
    """Whisperæ¨¡å‹ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""

    _instance = None
    _model = None
    _model_name = None

    def __new__(cls):
        """å•ä¾‹æ¨¡å¼å®ç°"""
        if cls._instance is None:
            cls._instance = super(WhisperManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        self.model_path = Path("models/whisper")
        self.model_name = CONFIG.get('whisper.model', 'small')
        self.language = CONFIG.get('whisper.language', 'zh')
        self.device = CONFIG.get('whisper.device', 'auto')
        self.temperature = CONFIG.get('whisper.temperature', 0.0)
        self._cancelled = False  # å–æ¶ˆæ ‡å¿—

        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        FileUtils.ensure_directory(self.model_path)

        # æ£€æŸ¥ç³»ç»Ÿé…ç½®
        self._setup_device()
        logger.info(f"ğŸ¤ Whisperç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (æ¨¡å‹: {self.model_name})")

    def _setup_device(self) -> None:
        """è®¾ç½®è®¾å¤‡ï¼ˆCPU/GPUï¼‰"""
        try:
            import torch

            if self.device == 'auto':
                if torch.cuda.is_available():
                    self.device = 'cuda'
                    gpu_name = torch.cuda.get_device_name(0)
                    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    logger.info(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_name} ({memory_gb:.1f}GB)")
                else:
                    self.device = 'cpu'
                    logger.info("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            elif self.device == 'cuda' and not torch.cuda.is_available():
                logger.warning("è¯·æ±‚ä½¿ç”¨CUDAä½†GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                self.device = 'cpu'

            # æ ¹æ®è®¾å¤‡ç±»å‹è°ƒæ•´æ¨¡å‹é€‰æ‹©å»ºè®®
            self._recommend_model()

        except ImportError:
            self.device = 'cpu'
            logger.warning("PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨CPUæ¨¡å¼")

    def _recommend_model(self) -> None:
        """æ ¹æ®ç³»ç»Ÿé…ç½®æ¨èæ¨¡å‹"""
        try:
            import torch

            if self.device == 'cpu':
                # CPUæ¨¡å¼æ¨èå°æ¨¡å‹
                recommended_models = ['tiny', 'base', 'small']
                if self.model_name not in recommended_models:
                    logger.warning(f"CPUæ¨¡å¼å»ºè®®ä½¿ç”¨è¾ƒå°æ¨¡å‹ï¼Œå½“å‰: {self.model_name}")
            else:
                # GPUæ¨¡å¼æ ¹æ®æ˜¾å­˜æ¨è
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

                if gpu_memory >= 8:
                    recommended_models = ['small', 'medium', 'large']
                elif gpu_memory >= 4:
                    recommended_models = ['base', 'small']
                else:
                    recommended_models = ['tiny', 'base']

                if self.model_name not in recommended_models:
                    logger.warning(f"GPUæ˜¾å­˜{gpu_memory:.1f}GBï¼Œå»ºè®®ä½¿ç”¨: {recommended_models}")

        except Exception as e:
            logger.warning(f"æ¨¡å‹æ¨èå¤±è´¥: {e}")

    def _running(self):
        """æ£€æŸ¥æ“ä½œæ˜¯å¦åº”è¯¥ç»§ç»­è¿è¡Œ"""
        # é»˜è®¤è¿”å›Trueï¼Œè¡¨ç¤ºç»§ç»­è¿è¡Œ
        # è¿™ä¸ªæ–¹æ³•å¯ä»¥è¢«å¤–éƒ¨çº¿ç¨‹è¦†ç›–æˆ–ä¿®æ”¹
        return not self._cancelled
        
    def cancel(self):
        """å–æ¶ˆå½“å‰æ“ä½œ"""
        logger.info("ğŸš« Whisperæ“ä½œè¢«å–æ¶ˆ")
        self._cancelled = True
        
    def reset_cancelled(self):
        """é‡ç½®å–æ¶ˆçŠ¶æ€"""
        self._cancelled = False

    def recommend_model_by_system(self) -> str:
        """æ ¹æ®è®¾å¤‡ä¸æ˜¾å­˜è¿”å›æ¨èæ¨¡å‹åç§°"""
        try:
            import torch
            if self.device == 'cpu':
                return 'base'
            mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
            if mem_gb >= 8:
                return 'medium'
            elif mem_gb >= 4:
                return 'small'
            else:
                return 'base'
        except Exception:
            return 'base'

    def load_model(self, model_name: str = None, force_reload: bool = False, progress_callback: Callable = None) -> bool:
        """
        åŠ è½½Whisperæ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§° (tiny, base, small, medium, large)
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # é‡ç½®å–æ¶ˆçŠ¶æ€
            self.reset_cancelled()
            
            if model_name is None:
                model_name = self.model_name
            
            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½ç›¸åŒæ¨¡å‹
            if not force_reload and self._model is not None and self._model_name == model_name:
                logger.debug(f"æ¨¡å‹ {model_name} å·²åŠ è½½")
                return True
            
            logger.info(f"ğŸ”„ åŠ è½½Whisperæ¨¡å‹: {model_name}")
            if progress_callback:
                progress_callback(10, f"å‡†å¤‡åŠ è½½æ¨¡å‹ {model_name}...")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            os.environ['WHISPER_MODEL_DIR'] = str(self.model_path)
            
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if not self._running():
                logger.info("æ¨¡å‹åŠ è½½è¢«å–æ¶ˆ")
                return False
            
            # å°è¯•é¢„æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„ä¸‹è½½
            model_file = self.model_path / f"{model_name}.pt"
            if model_file.exists():
                logger.info(f"ğŸ“ æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨: {model_file}")
                if progress_callback:
                    progress_callback(30, "æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œå‡†å¤‡åŠ è½½...")
                    # å³ä½¿æ–‡ä»¶å­˜åœ¨ï¼Œä»ç„¶æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                    if not self._running():
                        logger.info("æ¨¡å‹åŠ è½½è¢«å–æ¶ˆ")
                        return False
            else:
                logger.info(f"â¬‡ï¸ éœ€è¦ä¸‹è½½æ¨¡å‹: {model_name}")
                if progress_callback:
                    progress_callback(25, f"å¼€å§‹ä¸‹è½½æ¨¡å‹ {model_name}...")
                    # ä¸‹è½½å‰å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                    if not self._running():
                        logger.info("æ¨¡å‹ä¸‹è½½è¢«å–æ¶ˆ")
                        return False
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œè¿™æ ·å¯ä»¥åœ¨éœ€è¦æ—¶æ›´å¥½åœ°æ§åˆ¶
            import threading
            import queue
            result_queue = queue.Queue()
            
            def load_model_worker():
                try:
                    # åœ¨å·¥ä½œçº¿ç¨‹ä¸­æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                    if not self._running():
                        result_queue.put((None, "æ“ä½œå·²å–æ¶ˆ"))
                        return
                    
                    # ä¸‹è½½/åŠ è½½è¿‡ç¨‹ä¸­ç®€å•è¿›åº¦æç¤º
                    if progress_callback:
                        progress_callback(50, "æ¨¡å‹ä¸‹è½½/åŠ è½½ä¸­...")
                    # åŠ è½½æ¨¡å‹
                    model = whisper.load_model(
                        model_name,
                        device=self.device,
                        download_root=str(self.model_path)
                    )
                    if progress_callback:
                        progress_callback(90, "æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‡†å¤‡å°±ç»ª...")
                    result_queue.put((model, None))
                except Exception as e:
                    result_queue.put((None, str(e)))
            
            # å¯åŠ¨å·¥ä½œçº¿ç¨‹
            load_thread = threading.Thread(target=load_model_worker)
            load_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»ç¨‹åºç»“æŸæ—¶ä¼šè‡ªåŠ¨ç»ˆæ­¢
            load_thread.start()
            
            # ç­‰å¾…çº¿ç¨‹å®Œæˆï¼Œä½†å®šæœŸæ£€æŸ¥å–æ¶ˆçŠ¶æ€
            import time
            while load_thread.is_alive():
                # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€
                time.sleep(0.5)
                if not self._running():
                    logger.info("æ¨¡å‹åŠ è½½è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ")
                    # è™½ç„¶æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¸­æ–­whisper.load_modelï¼Œä½†è®¾ç½®å–æ¶ˆæ ‡å¿—
                    # å½“çº¿ç¨‹å®Œæˆæ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨ç»“æœ
                    self.cancel()
                    # ä»ç„¶ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œé¿å…èµ„æºæ³„æ¼
                    load_thread.join(timeout=2.0)
                    return False
            
            # è·å–åŠ è½½ç»“æœ
            model, error = result_queue.get()
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if not self._running():
                logger.info("æ¨¡å‹åŠ è½½å®Œæˆä½†æ“ä½œå·²è¢«å–æ¶ˆ")
                return False
            
            if error:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å–æ¶ˆå¯¼è‡´çš„é”™è¯¯
                if self._cancelled:
                    logger.info(f"æ¨¡å‹åŠ è½½è¢«å–æ¶ˆ: {error}")
                    return False
                else:
                    raise Exception(error)
            
            self._model = model
            self._model_name = model_name
            self.model_name = model_name
            
            logger.info(f"âœ… Whisperæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            if progress_callback:
                progress_callback(100, f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            return True
        
        except Exception as e:
            # åŒºåˆ†æ­£å¸¸å¼‚å¸¸å’Œå–æ¶ˆå¼‚å¸¸
            if self._cancelled:
                logger.info(f"æ¨¡å‹åŠ è½½è¢«å–æ¶ˆ: {e}")
                return False
            else:
                logger.error(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False

    def transcribe_audio(
        self,
        audio_path: Union[str, Path],
        language: str = None,
        temperature: float = None,
        progress_callback: Callable = None,
        max_retries: int = None
    ) -> Dict[str, Any]:
        """
        è½¬å†™éŸ³é¢‘ä¸ºæ–‡å­—

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç  (å¦‚: zh, en)
            temperature: æ¸©åº¦å‚æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰

        Returns:
            è½¬å†™ç»“æœå­—å…¸
        """
        try:
            # é‡ç½®å–æ¶ˆçŠ¶æ€
            self.reset_cancelled()

            # è®¾ç½®é‡è¯•æ¬¡æ•°
            if max_retries is None:
                max_retries = CONFIG.get('whisper.max_retries', 2)

            # éªŒè¯éŸ³é¢‘æ–‡ä»¶å­˜åœ¨
            audio_path = Path(audio_path)
            if not audio_path.exists():
                return {
                    'success': False,
                    'error': 'éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨',
                    'text': None
                }

            # è®¾ç½®å‚æ•°
            language = language or self.language
            temperature = temperature if temperature is not None else self.temperature

            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«: {audio_path}")
            logger.debug(f"å‚æ•° - è¯­è¨€: {language}, æ¸©åº¦: {temperature}, è®¾å¤‡: {self.device}, æœ€å¤§é‡è¯•: {max_retries}")

            # é‡è¯•æœºåˆ¶
            last_error = None
            retry_delay = CONFIG.get('whisper.retry_delay', 3)

            for attempt in range(max_retries + 1):  # +1 è¡¨ç¤ºç¬¬ä¸€æ¬¡å°è¯• + max_retriesæ¬¡é‡è¯•
                if attempt > 0:
                    logger.info(f"ğŸ”„ ç¬¬{attempt}æ¬¡é‡è¯•...")
                    if progress_callback:
                        progress_callback(5, f"ç¬¬{attempt}æ¬¡é‡è¯•...")

                    # é‡è¯•å»¶è¿Ÿ
                    if retry_delay > 0 and attempt < max_retries:
                        logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        if progress_callback:
                            progress_callback(5, f"ç­‰å¾… {retry_delay} ç§’...")
                        time.sleep(retry_delay)

                # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                if not self._running():
                    logger.info("è¯­éŸ³è¯†åˆ«è¢«å–æ¶ˆ")
                    return {
                        'success': False,
                        'error': 'æ“ä½œå·²å–æ¶ˆ',
                        'text': None
                    }

                # å°è¯•è½¬å†™
                result = self._transcribe_with_timeout(
                    audio_path, language, temperature, progress_callback, attempt
                )

                if result['success']:
                    logger.info(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸï¼Œè¯†åˆ«å­—æ•°: {len(result.get('text', ''))}")
                    return result
                else:
                    last_error = result.get('error', 'æœªçŸ¥é”™è¯¯')
                    # å¦‚æœæ˜¯å–æ¶ˆé”™è¯¯ï¼Œç«‹å³è¿”å›
                    if 'å–æ¶ˆ' in last_error:
                        return result
                    # è®°å½•é”™è¯¯å¹¶ç»§ç»­é‡è¯•
                    logger.warning(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {last_error}")

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")
            return {
                'success': False,
                'error': f'è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ã€‚é”™è¯¯: {last_error}',
                'text': None
            }

        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¼‚å¸¸: {e}")
            return {
                'success': False,
                'error': str(e),
                'text': None
            }

    def _transcribe_with_timeout(
        self,
        audio_path: Path,
        language: str,
        temperature: float,
        progress_callback: Callable,
        attempt: int
    ) -> Dict[str, Any]:
        """
        å¸¦è¶…æ—¶æœºåˆ¶çš„éŸ³é¢‘è½¬å†™ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç 
            temperature: æ¸©åº¦å‚æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            è½¬å†™ç»“æœå­—å…¸
        """
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if self._model is None:
                # ä¼ é€’è¿›åº¦å›è°ƒç»™æ¨¡å‹åŠ è½½ï¼Œä»¥è·å¾—ä¸€è‡´æ—¥å¿—å’Œå–æ¶ˆå¤„ç†
                if not self.load_model(progress_callback=progress_callback):
                    if self._cancelled:
                        return {
                            'success': False,
                            'error': 'æ“ä½œå·²å–æ¶ˆ',
                            'text': None
                        }
                    return {
                        'success': False,
                        'error': 'æ¨¡å‹æœªåŠ è½½',
                        'text': None
                    }

            # å‘é€å¼€å§‹è¿›åº¦
            if progress_callback:
                progress_callback(10, "æ­£åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶...")

            # ä½¿ç”¨çº¿ç¨‹åŒ…è£…è½¬å†™æ“ä½œä»¥æ”¯æŒè¶…æ—¶
            import threading
            import time

            result_container = {'result': None, 'error': None, 'completed': False}

            def transcribe_worker():
                try:
                    if progress_callback:
                        progress_callback(30, "æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")

                    # æ‰§è¡Œè½¬å†™
                    result = self._model.transcribe(
                        str(audio_path),
                        language=language,
                        temperature=temperature,
                        verbose=CONFIG.get('whisper.verbose', False),
                        fp16=(self.device == 'cuda'),
                        word_timestamps=False
                    )
                    result_container['result'] = result
                    result_container['completed'] = True

                except Exception as e:
                    result_container['error'] = str(e)
                    result_container['completed'] = True

            # å¯åŠ¨è½¬å†™çº¿ç¨‹
            transcribe_thread = threading.Thread(target=transcribe_worker)
            transcribe_thread.daemon = True
            transcribe_thread.start()

            # ç­‰å¾…è½¬å†™å®Œæˆï¼ŒåŒæ—¶æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            start_time = time.time()
            timeout = CONFIG.get('whisper.timeout', 600)  # 10åˆ†é’Ÿè¶…æ—¶ï¼ˆå·²æ›´æ–°ä¸º600ç§’ï¼‰

            while not result_container['completed']:
                if not self._running():
                    logger.info("è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ")
                    return {
                        'success': False,
                        'error': 'æ“ä½œå·²å–æ¶ˆ',
                        'text': None
                    }

                # æ£€æŸ¥è¶…æ—¶
                if time.time() - start_time > timeout:
                    logger.warning(f"è¯­éŸ³è¯†åˆ«è¶…æ—¶ ({timeout}ç§’)")
                    return {
                        'success': False,
                        'error': f'æ“ä½œè¶…æ—¶ ({timeout}ç§’)',
                        'text': None
                    }

                # æ›´æ–°è¿›åº¦
                elapsed = time.time() - start_time
                progress = min(30 + int((elapsed / timeout) * 60), 90)
                if progress_callback:
                    progress_callback(progress, f"æ­£åœ¨è¯†åˆ«è¯­éŸ³... ({elapsed:.0f}s)")

                time.sleep(0.5)  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡

            # æ£€æŸ¥ç»“æœ
            if result_container['error']:
                raise Exception(result_container['error'])

            result = result_container['result']

            # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if not self._running():
                logger.info("è¯­éŸ³è¯†åˆ«å®Œæˆä½†æ“ä½œå·²è¢«å–æ¶ˆ")
                return {
                    'success': False,
                    'error': 'æ“ä½œå·²å–æ¶ˆ',
                    'text': None
                }

            # å¤„ç†ç»“æœ
            text = result.get('text', '').strip()

            if not text:
                return {
                    'success': False,
                    'error': 'æœªè¯†åˆ«åˆ°è¯­éŸ³å†…å®¹',
                    'text': None
                }

            return {
                'success': True,
                'text': text,
                'language': result.get('language', language),
                'duration': self._estimate_audio_duration(result),
                'word_count': len(text),
                'model_used': self._model_name,
                'device_used': self.device,
                'attempt': attempt + 1  # è®°å½•æˆåŠŸæ—¶çš„å°è¯•æ¬¡æ•°
            }

        except Exception as e:
            logger.error(f"âŒ å•æ¬¡è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'text': None
            }

    def _estimate_audio_duration(self, result: Dict[str, Any]) -> Optional[float]:
        """ä¼°ç®—éŸ³é¢‘æ—¶é•¿"""
        try:
            segments = result.get('segments', [])
            if segments:
                return max(segment.get('end', 0) for segment in segments)
            return None
        except:
            return None

    def get_available_models(self) -> list:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            available = []
            for model_name in ['tiny', 'base', 'small', 'medium', 'large']:
                model_path = self.model_path / model_name
                if model_path.exists() or model_name == self._model_name:
                    available.append(model_name)
            return available
        except Exception as e:
            logger.error(f"è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {e}")
            return []

    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_name is None:
            model_name = self.model_name

        model_info = {
            'tiny': {
                'name': 'Tiny',
                'size_mb': 75,
                'speed': 'æå¿«',
                'accuracy': 'åŸºç¡€',
                'vram_mb': 1,
                'description': 'é€‚ç”¨äºå®æ—¶å¤„ç†ï¼Œå‡†ç¡®åº¦è¾ƒä½'
            },
            'base': {
                'name': 'Base',
                'size_mb': 142,
                'speed': 'å¾ˆå¿«',
                'accuracy': 'è‰¯å¥½',
                'vram_mb': 2,
                'description': 'å¹³è¡¡é€‰æ‹©ï¼Œæ—¥å¸¸ä½¿ç”¨æ¨è'
            },
            'small': {
                'name': 'Small',
                'size_mb': 466,
                'speed': 'å¿«é€Ÿ',
                'accuracy': 'é«˜',
                'vram_mb': 4,
                'description': 'é«˜ç²¾åº¦ï¼Œæ¨èç”¨äºæ­£å¼ç”¨é€”'
            },
            'medium': {
                'name': 'Medium',
                'size_mb': 1530,
                'speed': 'ä¸­ç­‰',
                'accuracy': 'å¾ˆé«˜',
                'vram_mb': 8,
                'description': 'ä¸“ä¸šçº§ç²¾åº¦ï¼Œéœ€è¦è¾ƒå¼ºç¡¬ä»¶'
            },
            'large': {
                'name': 'Large',
                'size_mb': 2950,
                'speed': 'æ…¢é€Ÿ',
                'accuracy': 'æœ€é«˜',
                'vram_mb': 16,
                'description': 'æœ€é«˜ç²¾åº¦ï¼Œå¤„ç†æ—¶é—´è¾ƒé•¿'
            }
        }

        return model_info.get(model_name, {})

    def download_model(self, model_name: str, progress_callback: Callable = None) -> bool:
        """
        ä¸‹è½½Whisperæ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ“¥ ä¸‹è½½Whisperæ¨¡å‹: {model_name}")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self._model_exists(model_name):
                logger.info(f"æ¨¡å‹ {model_name} å·²å­˜åœ¨")
                return True

            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['WHISPER_MODEL_DIR'] = str(self.model_path)

            # æ¨¡æ‹Ÿè¿›åº¦å›è°ƒï¼ˆå®é™…whisperæ²¡æœ‰è¿›åº¦å›è°ƒï¼‰
            if progress_callback:
                progress_callback(10, f"å‡†å¤‡ä¸‹è½½ {model_name} æ¨¡å‹...")
                progress_callback(30, f"æ­£åœ¨ä¸‹è½½ {model_name} æ¨¡å‹...")
                progress_callback(60, f"ä¸‹è½½ {model_name} æ¨¡å‹ä¸­...")
                progress_callback(90, f"å®Œæˆä¸‹è½½ {model_name} æ¨¡å‹...")

            # åŠ è½½æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
            whisper.load_model(model_name, download_root=str(self.model_path))

            if progress_callback:
                progress_callback(100, f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")

            logger.info(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _model_exists(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        try:
            model_file = self.model_path / f"{model_name}.pt"
            return model_file.exists()
        except:
            return False

    def get_current_model(self) -> Optional[str]:
        """è·å–å½“å‰åŠ è½½çš„æ¨¡å‹"""
        return self._model_name

    def unload_model(self) -> None:
        """å¸è½½å½“å‰æ¨¡å‹"""
        try:
            if self._model is not None:
                import torch
                del self._model
                self._model = None
                self._model_name = None

                if self.device == 'cuda':
                    torch.cuda.empty_cache()

                logger.info("ğŸ”„ Whisperæ¨¡å‹å·²å¸è½½")
        except Exception as e:
            logger.warning(f"å¸è½½æ¨¡å‹å¤±è´¥: {e}")

    def get_system_requirements(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿè¦æ±‚ä¿¡æ¯"""
        try:
            import torch

            requirements = {
                'python_version': platform.python_version(),
                'pytorch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'current_device': self.device,
                'loaded_model': self._model_name
            }

            if torch.cuda.is_available():
                requirements.update({
                    'cuda_version': torch.version.cuda,
                    'device_count': torch.cuda.device_count(),
                    'current_device_name': torch.cuda.get_device_name(0),
                    'gpu_memory_gb': round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2)
                })

            return requirements

        except Exception as e:
            logger.error(f"è·å–ç³»ç»Ÿè¦æ±‚å¤±è´¥: {e}")
            return {'error': str(e)}

    def cleanup_models(self, keep_models: list = None) -> None:
        """æ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
        try:
            if keep_models is None:
                keep_models = [self._model_name]

            for model_dir in self.model_path.iterdir():
                if model_dir.is_dir() and model_dir.name not in keep_models:
                    import shutil
                    shutil.rmtree(model_dir)
                    logger.info(f"åˆ é™¤æœªä½¿ç”¨æ¨¡å‹: {model_dir.name}")

        except Exception as e:
            logger.warning(f"æ¸…ç†æ¨¡å‹æ–‡ä»¶å¤±è´¥: {e}")

    def test_transcription(self, audio_path: Union[str, Path] = None) -> Dict[str, Any]:
        """æµ‹è¯•è¯­éŸ³è¯†åˆ«åŠŸèƒ½"""
        try:
            if audio_path is None:
                # åˆ›å»ºæµ‹è¯•éŸ³é¢‘ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
                return self._create_test_transcription()

            return self.transcribe_audio(audio_path)

        except Exception as e:
            return {
                'success': False,
                'error': f'è¯­éŸ³è¯†åˆ«æµ‹è¯•å¤±è´¥: {str(e)}'
            }

    def _create_test_transcription(self) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•è½¬å†™ç»“æœ"""
        return {
            'success': True,
            'text': "è¿™æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æµ‹è¯•ã€‚å¦‚æœèƒ½çœ‹åˆ°è¿™æ®µæ–‡å­—ï¼Œè¯´æ˜Whisperå·¥ä½œæ­£å¸¸ã€‚",
            'language': 'zh',
            'test_mode': True,
            'word_count': 28
        }

if __name__ == "__main__":
    # æµ‹è¯•Whisperç®¡ç†å™¨
    print("=== Whisperç®¡ç†å™¨æµ‹è¯• ===")

    manager = WhisperManager()

    print(f"å½“å‰è®¾å¤‡: {manager.device}")
    print(f"æ¨¡å‹ç›®å½•: {manager.model_path}")
    print(f"æ¨èæ¨¡å‹: {manager.get_model_info()}")

    # æµ‹è¯•æ¨¡å‹ä¿¡æ¯
    print("\nå¯ç”¨æ¨¡å‹:")
    for model in ['tiny', 'base', 'small', 'medium', 'large']:
        info = manager.get_model_info(model)
        print(f"  {model}: {info.get('name')} - {info.get('description')}")

    # æµ‹è¯•ç³»ç»Ÿè¦æ±‚
    print(f"\nç³»ç»Ÿè¦æ±‚: {manager.get_system_requirements()}")